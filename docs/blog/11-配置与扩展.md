# 11 - 配置与扩展

本篇汇总常用配置方式（环境变量、模型与后端、报告行为），并简要说明如何扩展（RAG、OpenClaw/IM 等），方便按需调参和二次开发。

---

## 一、环境变量（常用）

| 变量 | 含义 | 默认 |
|------|------|------|
| `OLLAMA_MODEL` | Ollama 模型名 | qwen2.5:3b |
| `LLM_BACKEND` | 后端：ollama / deepseek / openai | 无 key 时为 ollama |
| `DEEPSEEK_API_KEY` | DeepSeek API Key | — |
| `OPENAI_API_KEY` | OpenAI API Key | — |
| `LLM_TEMPERATURE` | 采样温度，越低输出越稳定 | 0.3 |
| `LLM_MAX_TOKENS` | 单次回复最大 token | 不设 |
| `LLM_TIMEOUT` | 请求超时（秒） | 120 |
| `DEEP_PARALLEL` | 深度分析是否并行，0=顺序 | 1 |
| `STOCK_AGENT_MEMORY_DIR` | 记忆存储目录（JSONL） | 项目下 data/memory |

- **本地免费**：不设任何 API Key，默认用 Ollama；需先安装 Ollama 并拉取模型，例如 `ollama pull qwen2.5:3b`。  
- **换模型**：`export OLLAMA_MODEL=qwen2.5:7b` 再启动；或设 `DEEPSEEK_API_KEY` / `OPENAI_API_KEY` 并设 `LLM_BACKEND=deepseek` / `openai`。  
- **调行为**：温度、max_tokens 等在 `config/llm_config.py` 中可读环境变量，`llm.ask_llm` 会使用这些默认值。

---

## 二、模型与后端（llm.py）

- **Ollama**：`base_url=http://localhost:11434/v1`，无需 API Key；模型名由 `OLLAMA_MODEL` 控制。  
- **DeepSeek**：设 `DEEPSEEK_API_KEY`，`base_url=https://api.deepseek.com`，常用模型如 `deepseek-chat`。  
- **OpenAI**：设 `OPENAI_API_KEY`，模型如 `gpt-4o-mini`；若用第三方兼容 API，需在代码里改 `base_url`。  

选择逻辑：优先看 `LLM_BACKEND`；若未设则根据「已配置的 Key」推断（有 DeepSeek key 且未指定 ollama 则用 DeepSeek，以此类推）。  
超时、连接错误、429/401 等会在 `llm.py` 里转成可读的 `RuntimeError` 提示（如「请先启动 Ollama」「API 配额不足」等）。

---

## 三、报告与 Prompt 可调点

- **综合 Prompt 的 10 项格式与说明**：在 `agents/full_analysis._build_prompt` 里，可改角色描述、时间范围描述、以及 10 项的说明文字。  
- **深度分析 Prompt 模板**：在 `agents/prompts` 与 `chains/chains` 里，可改各类深度问题的表述。  
- **报告 HTML 与筛选**：`report/build_html.py` 里可改标题样式、筛选项、排序选项、评分定性映射（`_score_interpretation`）等。  
- **选股池与代码规范**：`config/tickers.py` 里可增删静态列表、改 `get_report_tickers` 逻辑（例如接动态接口）。  

---

## 四、扩展方向（简要）

- **RAG**：把历史报告摘要、新闻摘要、财报解读等做成文档，切块后做 embedding 存向量库（如 Chroma）；分析时按「标的/行业/问题」检索 TopK 再拼进 Prompt。思路与数据来源见 `docs/RAG文档建立与数据来源.md`。  
- **OpenClaw / iMessage**：通过外部消息（如「帮我跑一份报告发到微信」）触发报告生成或推送；安装与接入步骤见 `docs/OpenClaw安装与iMessage(imsg)接入.md`、`docs/OpenClaw集成说明.md`。  
- **更多数据源**：研报、公告、宏观数据等需自行对接 API 或爬虫；可在对应 agent（如 news、fundamental）或新建模块中拉取，再拼进 Prompt 或单独展示。  
- **多轮对话记忆**：当前记忆是「按标的+分析类型」的 JSONL，不是「按用户会话」的对话历史；若要「连续问答」形态，可在此基础上加 LangChain 的 `ConversationBufferWindowMemory` 等，与现有 memory_store 并存。

---

## 五、小结

- **配置**：环境变量控制模型、后端、超时、温度、深度是否并行、记忆目录；`config/llm_config` 与 `llm.py` 是入口。  
- **扩展**：RAG、OpenClaw、新数据源、对话记忆等都有文档或代码入口可循；按需选一块先做最小可行再迭代。  

本系列博客到这里告一段落；若你希望某一块再拆成「更细的步骤」或「带代码片段」，可以指定章节再补一篇。
