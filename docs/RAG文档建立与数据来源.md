# RAG 文档建立思路与数据来源

本文说明在 stock-agent 里如何建立 RAG（检索增强生成）文档、从哪里捞数据、以及和现有「记忆」的关系。

---

## 一、RAG 是什么、在本项目里的价值

- **RAG**：先把文档切成片段、做向量索引；用户/分析时按「问题或当前标的」做语义检索，把检索到的片段拼进 LLM 的 prompt，让回答有据可查、减少幻觉。
- **在本项目的价值**：
  - **历史结论复用**：分析 AAPL 时，可检索「同行业（如 MSFT、GOOGL）或同类型」的过往分析结论，保持口径一致、减少重复分析。
  - **知识库增强**：把研报、财报解读、新闻摘要等存成文档，分析时按「标的/行业/主题」检索，提升综合质量。
  - **长文档利用**：年报、招股书、深度研报太长无法整篇进 prompt，切块后通过 RAG 按需取用。

当前项目的「记忆」是 **按 ticker + analysis_type 精确检索**（`memory_store.jsonl`），**不是**按语义相似检索；RAG 是**按语义相似**检索，两者可并存。

---

## 二、建立 RAG 文档的整体思路

```
数据来源 → 文档化(结构化/半结构化) → 切分(Chunk) → 向量化(Embedding) → 写入向量库
                                                                              ↓
分析/提问时： 问题或当前标的描述 → 向量检索 TopK → 拼进 Prompt → LLM 生成
```

1. **定数据源**：先决定「要增强哪类知识」（历史报告、新闻、财报、研报等），见第三节。
2. **文档化**：把原始数据变成「一条条可检索的文本」（如：标题 + 摘要 + 来源 + 时间）。
3. **切分 (Chunk)**：长文按段落或按语义块切（如 300–600 字/块，可重叠滑动窗口），保证每块能单独做 embedding。
4. **向量化**：用 Embedding 模型把每块转成向量；可选 Ollama 本地模型或 sentence-transformers。
5. **写入向量库**：如 Chroma、FAISS、Milvus 等，按需选本地或云端。
6. **检索与拼进 Prompt**：分析某标的时，用「标的 + 行业 + 当前问题」生成查询向量，检索 TopK 条，拼成「参考文档」再调用 LLM。

---

## 三、数据从哪里捞取（按来源分类）

### 1. 项目内部（已有或易得）

| 数据 | 来源 | 说明 |
|------|------|------|
| **历史分析结果** | `data/memory/memory_store.jsonl` | 每行一条：ticker、analysis_type、content、ts。可直接把 `content` 当文档，或先做摘要再入 RAG。 |
| **报告卡片摘要** | 每次 `/report` 生成的 cards | 每只标的的 core_conclusion、score、action、score_reason 等，可拼成「标的 + 结论」短文，按 ticker/市场/行业打标签便于过滤。 |
| **深度分析摘要** | 深度报告中的 5 类摘要 | 基本面深度、护城河、同行对比、空头视角、叙事变化。可每条存为一段文档，metadata 含 ticker、类型、时间。 |

**落地建议**：  
- 定时/报告生成后，把 `memory_store` 新增条或报告卡片「摘要文本」写入向量库；  
- 检索时可按 ticker/行业过滤后再做语义相似（例如「同行业」优先）。

---

### 2. 行情与新闻（已接入，可再加工）

| 数据 | 来源 | 说明 |
|------|------|------|
| **新闻标题 + 摘要** | 现有 `agents/news.py`（yfinance 新闻） | 已有 `get_news_summary_llm` 生成的 1–2 句摘要，可把「ticker + 日期 + 摘要」当一条文档入 RAG，供「近期消息面」检索。 |
| **财报摘要** | `agents/fundamental.py`（yfinance financials） | 已有 `get_financials_interpretation` 的 2–3 句解读，可「ticker + 财报解读」入 RAG，供「基本面/估值」检索。 |

**落地建议**：  
- 在跑报告或单标的分析时，把「新闻摘要」「财报解读」结果同步写入向量库（带 ticker、日期、类型）；  
- 后续分析同一标的或同行业时，可检索「该标的/同行业」的近期新闻与财报解读。

---

### 3. 研报与公告（需额外数据源）

| 数据 | 可能来源 | 说明 |
|------|----------|------|
| **券商研报** | 东方财富、同花顺、Wind、开源数据接口等 | 标题 + 摘要 + 评级 + 标的，可爬取或买接口，按「标的/行业」入 RAG。 |
| **公告/招股书** | 巨潮、交易所、SEC EDGAR、公司 IR 页 | 公告摘要或全文切块；招股书 PDF 解析后按节切块，metadata 含公司、类型、日期。 |
| **宏观/行业研报** | 券商、第三方研报平台 | 按行业或主题切块，检索时用「行业名 + 当前标的」增强上下文。 |

**注意**：爬虫需遵守网站条款与合规；商业数据需采购接口。

---

### 4. 公开知识（可选）

| 数据 | 来源 | 说明 |
|------|------|------|
| **公司简介/业务** | Wikipedia、公司官网、yfinance info | 短文本，可直接作文档或与财报解读合并。 |
| **行业百科/术语** | 自建或公开语料 | 解释技术指标、行业术语，检索时用于「概念解释」类问题。 |

---

## 四、技术选型建议（简要）

- **向量库**：本地优先可用 **Chroma**（易用、够用）；要高性能可 **FAISS**；有分布式需求再考虑 Milvus 等。  
- **Embedding**：  
  - 本地：**Ollama** 跑 `nomic-embed-text` 或类似模型，与现有 LLM 一致；  
  - 或 **sentence-transformers**（如 `paraphrase-multilingual-MiniLM-L12-v2`）做中文+英文混合。  
- **切分**：按段落或按固定长度（如 400 字）+ 重叠 50 字，避免语义被截断；每条带 metadata（ticker、类型、时间、来源）。  
- **检索**：先按 metadata 过滤（如 ticker / 行业 / 类型），再按向量相似度取 TopK（如 5–10 条），拼成「参考文档」再调 LLM。

---

## 五、与现有 memory_store 的关系

| 维度 | memory_store（现有） | RAG 文档（新增） |
|------|----------------------|-------------------|
| 检索方式 | 按 **ticker + analysis_type** 精确取最近 N 条 | 按 **语义相似** 检索（同行业、同主题、相似问题） |
| 典型用途 | 「上次/历史分析」对比、与上次对比、复盘 | 同行业结论、研报观点、新闻/财报摘要等「知识」增强 |
| 存储 | JSONL 追加，按 key 截断 | 向量库 + 可选 metadata 过滤 |
| 是否共存 | 是 | 两套可同时用：memory 管「该标的上次结果」，RAG 管「相关知识与同行业结论」 |

建议：**先用自己的数据建 RAG**（memory 条、报告卡片摘要、新闻摘要、财报解读），验证效果后再接外部研报/公告。

---

## 六、最小可行方案（MVP）

1. **数据**：仅用 `memory_store.jsonl` 里的 `content`（或摘要）+ 报告生成时的「核心结论 + 评分理由」文本。  
2. **管道**：  
   - 脚本：读 JSONL / 报告结果 → 按 ticker + 类型切块或整条当文档 → 调用 Ollama 或 sentence-transformers 做 embedding → 写入 Chroma。  
   - 分析时：当前 ticker + 行业（若有）→ 向量检索 TopK → 拼成「参考：…」再调 `ask_llm`。  
3. **效果**：同一标的或同行业再次分析时，LLM 能看到「之前类似结论」，一致性和可解释性会更好。

若你愿意，下一步可以在项目里加一个 `rag/` 目录：  
- `rag/build_index.py`：从 memory_store + 报告结果建索引；  
- `rag/retrieve.py`：按 query/ticker 检索，返回若干条文本供拼 prompt；  
- 在 `full_analysis` 或深度分析里可选「是否启用 RAG 上下文」。  

需要的话我可以按你当前仓库结构写出上述脚本的草稿（含依赖与配置要点）。
